# app.py
# Hi-PolicyLens | Streamlit ë²„ì „ (ê³µê³µê¸°ê´€ 3ê³³ ì „ìš©)
# - motie.go.kr (RSS), cbp.gov (RSS), echa.europa.eu/legislation (HTML íŒŒì‹±)
# - ê²€ìƒ‰(ë¹ ë¦„): ëª©ë¡ë§Œ ë¶ˆëŸ¬ì˜¤ê¸°
# - ìš”ì•½/ëª¨ë‘ ìš”ì•½: ë¬¸ì„œë³„ Potens.AI ì •ê·œí™”

import os, json, time, requests, feedparser, streamlit as st
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup

st.set_page_config(page_title="Hi-PolicyLens | ê·œì œ ë¹„êµ ë¶„ì„", layout="wide")
BRAND_ORANGE = "#dc8d32"; BRAND_NAVY = "#0f2e69"
st.markdown(f"""
<style>
:root {{ --brand-orange:{BRAND_ORANGE}; --brand-navy:{BRAND_NAVY}; --border:#e5e7eb; --surface:#f8f9fa; }}
h1, h2, h3, h4 {{ color: {BRAND_NAVY}; }}
.small-muted {{ color:#6b7280; font-size:13px }}
.chip {{ display:inline-block; padding:4px 10px; background:#f3f4f6; border:1px solid var(--border);
  border-radius:20px; font-size:12px; color:#6b7280; font-weight:500; }}
.btn-link {{ background:{BRAND_ORANGE}; color:white; text-decoration:none; padding:6px 10px; border-radius:6px; font-weight:600; }}
.card {{ background:#fff; border:1px solid var(--border); border-radius:12px; padding:16px; margin-top:16px; }}
hr {{ border-color:#eee }}
</style>
""", unsafe_allow_html=True)

# --- Secrets / ENV ---
POTENS_API_KEY = st.secrets.get("POTENS_API_KEY", os.getenv("POTENS_API_KEY", "PUT_YOUR_POTENS_API_KEY_HERE"))
POTENS_ENDPOINT = st.secrets.get("POTENS_ENDPOINT", os.getenv("POTENS_ENDPOINT", "https://ai.potens.ai/api/chat"))

SECTORS = ["solar","wind","hydro","nuclear"]
SECTOR_LABELS = {"solar":"íƒœì–‘ê´‘","wind":"í’ë ¥","hydro":"ìˆ˜ë ¥","nuclear":"ì›ìë ¥"}

def region_from_url(url: str) -> str:
    try:
        host = urlparse(url).hostname or ""
        if host.endswith("cbp.gov"): return "ë¶ë¯¸"
        if host.endswith("motie.go.kr"): return "ì•„ì‹œì•„"
        if host.endswith("europa.eu"): return "ìœ ëŸ½"
    except: pass
    return ""

def html_to_text(html: str) -> str:
    soup = BeautifulSoup(html or "", "html.parser")
    for t in soup(["script","style","noscript"]): t.extract()
    return " ".join(soup.get_text(" ").split())

def fetch_html(url: str, timeout=20) -> str:
    try:
        r = requests.get(url, timeout=timeout, allow_redirects=True,
                         headers={"User-Agent":"Hi-PolicyLens/Streamlit"})
        if 200 <= r.status_code < 300:
            r.encoding = r.apparent_encoding or r.encoding
            return r.text
    except: return ""
    return ""

def extract_json_array(s: str):
    if not s: return None
    t = s.strip().replace("```json","").replace("```","").strip()
    i, j = t.find("["), t.rfind("]")
    if i!=-1 and j!=-1 and j>i:
        try:
            arr = json.loads(t[i:j+1])
            return arr if isinstance(arr,list) else None
        except: pass
    try:
        arr = json.loads(t)
        return arr if isinstance(arr, list) else None
    except: return None

def build_prompt(text: str, origin: str) -> str:
    clipped = (text or "")[:6000]
    return f"""ì—­í• : êµ­ì œ ê·œì œ ë¶„ì„ê°€
ëª©í‘œ: ì•„ë˜ ì›ë¬¸ì—ì„œ "ì‹ ì¬ìƒì—ë„ˆì§€ ê´€ë ¨ ê·œì œ"ë§Œ ì¶”ì¶œí•˜ì—¬ JSON ë°°ì—´ë¡œ ì •ê·œí™”.

ìŠ¤í‚¤ë§ˆ:
[
  {{
    "jurisdiction": "êµ­ê°€/ê¸°ê´€/ì§€ì—­",
    "law_or_policy": "ë²•/ì •ì±…/ì§€ì¹¨ ëª…",
    "effective_date": "YYYY-MM-DD ë˜ëŠ” ë¯¸ìƒ",
    "requirements": ["í•µì‹¬ ìš”ê±´1","í•µì‹¬ ìš”ê±´2"],
    "reporting": "ë³´ê³ /ì‹ ê³  ì£¼ê¸° ë˜ëŠ” ë°©ì‹(ë¯¸ìƒì´ë©´ 'N/A')",
    "incentives": ["ì„¸ì œ/ë³´ì¡° ë“±"],
    "penalties": ["ë¯¸ì´í–‰ì‹œ ì œì¬"],
    "source": "ì›ë¬¸ URL"
  }}
]

[ì›ë¬¸ ì¶œì²˜] {origin}
[ì›ë¬¸]
{clipped}

ë°˜ë“œì‹œ **ìˆœìˆ˜í•œ ìœ íš¨ JSON ë°°ì—´([])**ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
- ë§ˆí¬ë‹¤ìš´/ì„¤ëª…/ì½”ë“œíœìŠ¤/ì£¼ì„/í…ìŠ¤íŠ¸ ê¸ˆì§€
- JSON ì™¸ ë¬¸ìë¥¼ í¬í•¨í•˜ì§€ ë§ ê²ƒ
- ë¶ˆí™•ì‹¤í•˜ë©´ ë¹ˆ ë°°ì—´([]) ë°˜í™˜"""

def normalize_with_ai(text: str, origin_url: str):
    if not POTENS_API_KEY or POTENS_API_KEY.startswith("PUT_"):  # í‚¤ ë¯¸ì„¤ì • ì‹œ í´ë°±
        return []
    prompt = build_prompt(text, origin_url)
    try:
        r = requests.post(
            POTENS_ENDPOINT,
            headers={"Authorization": f"Bearer {POTENS_API_KEY}",
                     "Content-Type":"application/json","Accept":"application/json"},
            json={"prompt": prompt}, timeout=40
        )
        body = r.text or ""
        if r.headers.get("content-type","").startswith("application/json"):
            try:
                j = r.json()
                body = j.get("response") or j.get("text") or j.get("content") or body
            except: pass
    except: return []
    arr = extract_json_array(body) or []
    out = []
    for it in arr:
        out.append({
            "jurisdiction": it.get("jurisdiction",""),
            "law_or_policy": it.get("law_or_policy",""),
            "effective_date": it.get("effective_date","N/A"),
            "requirements": it.get("requirements",[]) if isinstance(it.get("requirements",[]),list) else [],
            "reporting": it.get("reporting","N/A"),
            "incentives": it.get("incentives",[]) if isinstance(it.get("incentives",[]),list) else [],
            "penalties": it.get("penalties",[]) if isinstance(it.get("penalties",[]),list) else [],
            "source": it.get("source", origin_url)
        })
    return out

def key_of(item: dict) -> str:
    return f"{item.get('jurisdiction','')}|{item.get('law_or_policy','')}"

# -----------------------------
# ê³µê³µê¸°ê´€ 3ê³³ ì†ŒìŠ¤ ì •ì˜
# -----------------------------
SOURCES = [
    # 1) MOTIE ë³´ë„ìë£Œ RSS
    {"type":"rss", "url":"https://www.motie.go.kr/rss/rssView.do?bbs_cd_n=81"},
    # 2) CBP ë¬´ì—­ ê³µì§€ RSS
    {"type":"rss", "url":"https://www.cbp.gov/rss/trade.xml"},
    # 3) ECHA Legislation (HTML ëª©ë¡ íŒŒì‹±)
    {"type":"html", "url":"https://echa.europa.eu/legislation"}
]

def fetch_from_rss(url: str):
    feed = feedparser.parse(url)
    out = []
    for e in getattr(feed, "entries", []) or []:
        if not getattr(e,"title",None) or not getattr(e,"link",None): continue
        out.append({
            "title": e.title,
            "link": e.link,
            "pubDate": getattr(e,"published","") or getattr(e,"updated",""),
            "source": url,
            "region": region_from_url(e.link)
        })
    return out

def fetch_from_echa_legislation(url: str):
    """ECHA ë²•ë ¹ í˜ì´ì§€ì—ì„œ ëª©ë¡í˜• ë§í¬/ì œëª©ì„ ê¸ì–´ì˜¨ë‹¤."""
    html = fetch_html(url)
    if not html: return []
    soup = BeautifulSoup(html, "html.parser")
    out = []
    # í˜ì´ì§€ êµ¬ì¡°ê°€ ë°”ë€” ìˆ˜ ìˆìœ¼ë¯€ë¡œ aíƒœê·¸ ì „ìˆ˜ ê²€ì‚¬ í›„ ë²•ë ¹/ì§€ì¹¨ ê´€ë ¨ ì„¹ì…˜ë§Œ ì·¨ì‚¬
    anchors = soup.select("a")
    for a in anchors:
        title = (a.get_text(strip=True) or "")
        href = a.get("href") or ""
        if not title or not href: continue
        # ì ˆëŒ€ê²½ë¡œí™”
        link = urljoin(url, href)
        # ê±°ì¹œ í•„í„°: ë‚´ë¶€ ë¬¸ì„œ/ë²•ë ¹ ìƒì„¸ë¡œ ì´ì–´ì§€ëŠ” ê²ƒ ìš°ì„ 
        if "legislation" in link or "regulation" in link or "directive" in link or "law" in link:
            out.append({
                "title": title,
                "link": link,
                "pubDate": "",  # í˜ì´ì§€ì— ë‚ ì§œê°€ ì¼ê´€ì ì´ì§€ ì•ŠìŒ
                "source": url,
                "region": region_from_url(link)
            })
    # ì¤‘ë³µ ì œê±°(ë§í¬ ê¸°ì¤€)
    seen = set(); uniq=[]
    for x in out:
        if x["link"] in seen: continue
        seen.add(x["link"]); uniq.append(x)
    # ì œëª© ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ë„ˆë¬´ ì§§ì€ ë…¸ì´ì¦ˆ ì œê±°
    uniq = [x for x in uniq if len(x["title"]) >= 8]
    return uniq[:40]

def fetch_feed_list(query: str):
    entries = []
    # ë””ë²„ê¹… í‘œì‹œìš© ë°•ìŠ¤
    dbg = st.empty()
    logs = []
    for src in SOURCES:
        kind, url = src["type"], src["url"]
        try:
            logs.append(f"ğŸ” Fetch: {kind.upper()} - {url}")
            if kind == "rss":
                items = fetch_from_rss(url)
            else:
                items = fetch_from_echa_legislation(url)
            logs.append(f"   â†’ {len(items)} items")
            entries.extend(items)
        except Exception as ex:
            logs.append(f"   âŒ Error: {ex}")
        dbg.write("\n".join(logs))
    # í•„í„° + ì •ë ¬ + ìƒí•œ
    q = (query or "").lower().strip()
    if q: entries = [x for x in entries if q in (x["title"] or "").lower()]
    entries.sort(key=lambda x: str(x["pubDate"]), reverse=True)
    return entries[:60]

# -----------------------------
# ì„¸ì…˜ ìƒíƒœ
# -----------------------------
if "list_rows" not in st.session_state: st.session_state["list_rows"] = []
if "normalized_rows" not in st.session_state: st.session_state["normalized_rows"] = []
if "prev_normalized_rows" not in st.session_state: st.session_state["prev_normalized_rows"] = []

# -----------------------------
# í—¤ë”/íˆ´ë°”
# -----------------------------
st.title("Hi-PolicyLens")
st.caption("êµ­ë‚´ì™¸ ê·œì œ ì°¨ì´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¹„êµí•˜ì—¬ íˆ¬ì ë¦¬ìŠ¤í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")
col1, col2, col3, col4 = st.columns([1,2,1,1])
with col1:
    sector = st.selectbox("ì„¹í„°", SECTORS, index=0, format_func=lambda s: SECTOR_LABELS.get(s,s))
with col2:
    query = st.text_input("í•„í„° í‚¤ì›Œë“œ(ì˜ˆ: renewable, RPS, FIT ë“±)", value="")
with col3:
    run_btn = st.button("ê²€ìƒ‰(ë¹ ë¦„)", use_container_width=True)
with col4:
    reset_btn = st.button("ì´ˆê¸°í™”", use_container_width=True)
st.divider()

if run_btn:
    st.session_state["prev_normalized_rows"] = st.session_state.get("normalized_rows", []).copy()
    st.session_state["normalized_rows"] = []
    with st.spinner("ê³µê³µê¸°ê´€ 3ê³³ì—ì„œ ëª©ë¡ ìˆ˜ì§‘ ì¤‘â€¦"):
        rows = fetch_feed_list(query)
        st.session_state["list_rows"] = rows
        if not rows:
            st.info("ìˆ˜ì§‘ëœ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
if reset_btn:
    st.session_state["list_rows"] = []
    st.session_state["normalized_rows"] = []
    st.session_state["prev_normalized_rows"] = []

# -----------------------------
# íƒ­
# -----------------------------
tab1, tab2 = st.tabs(["ê°œìš”", "ìš”ì•½/ë¹„êµ"])

with tab1:
    st.subheader("êµ­ê°€ë³„ ê·œì œ ë¬¸ì„œ ëª©ë¡")
    st.caption("ë¬¸ì„œë³„ **ìš”ì•½**ì„ ëˆŒëŸ¬ ì •ê·œí™”í•˜ì„¸ìš”. (ECHAëŠ” ëª©ë¡ í˜ì´ì§€ â†’ ì„¸ë¶€ ë¬¸ì„œë¡œ ì´ë™í•´ ìš”ì•½ë©ë‹ˆë‹¤)")
    rows = st.session_state.get("list_rows", [])
    if not rows:
        st.info("ê²€ìƒ‰(ë¹ ë¦„)ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
    else:
        cols = st.columns([1,4,1])
        with cols[1]:
            all_sum = st.button("ëª¨ë‘ ìš”ì•½(ì•ˆì „ëª¨ë“œ)", type="primary")
        if all_sum:
            prog = st.progress(0, text="ëª¨ë‘ ìš”ì•½ ì¤‘â€¦")
            acc = []
            for i, r in enumerate(rows):
                html = fetch_html(r["link"])
                text = html_to_text(html)[:6000]
                items = normalize_with_ai(text, r["link"])
                if not items:
                    items = [{
                        "jurisdiction":"", "law_or_policy": r["title"], "effective_date":"N/A",
                        "requirements":[], "reporting":"N/A", "incentives":[], "penalties":[], "source": r["link"]
                    }]
                for it in items:
                    acc.append({ **it, "region": r["region"], "link": r["link"], "title": r["title"] })
                prog.progress((i+1)/len(rows))
                time.sleep(0.2)
            st.session_state["normalized_rows"] = acc
            st.success(f"ìš”ì•½/ì •ê·œí™” ì™„ë£Œ: {len(acc)}ê°œ")
        # ëª©ë¡ ë Œë” + ê°œë³„ ìš”ì•½
        for i, r in enumerate(rows):
            with st.container():
                c1, c2, c3, c4 = st.columns([1,4,3,1])
                c1.markdown(f"<span class='chip'>{r['region'] or ''}</span>", unsafe_allow_html=True)
                c2.write(r["title"])
                if c3.button("ìš”ì•½", key=f"sum_{i}"):
                    with st.spinner("ìš”ì•½/ì •ê·œí™” ì¤‘â€¦"):
                        html = fetch_html(r["link"])
                        text = html_to_text(html)[:6000]
                        items = normalize_with_ai(text, r["link"])
                        if not items:
                            items = [{
                                "jurisdiction":"", "law_or_policy": r["title"], "effective_date":"N/A",
                                "requirements":[], "reporting":"N/A", "incentives":[], "penalties":[], "source": r["link"]
                            }]
                        for it in items:
                            st.session_state["normalized_rows"].append({ **it, "region": r["region"], "link": r["link"], "title": r["title"] })
                        st.success(f"ìš”ì•½ ì™„ë£Œ ({len(items)}ê°œ)")
                c4.markdown(f"<a class='btn-link' href='{r['link']}' target='_blank'>ì›ë¬¸</a>", unsafe_allow_html=True)
            st.markdown("<hr/>", unsafe_allow_html=True)

with tab2:
    st.subheader("ìš”ì•½ ê²°ê³¼ / ë¹„êµ")
    norm = st.session_state.get("normalized_rows", [])
    prev = st.session_state.get("prev_normalized_rows", [])

    if not norm:
        st.info("ìš”ì•½/ì •ê·œí™”ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê°œìš” íƒ­ì—ì„œ [ìš”ì•½] ë˜ëŠ” [ëª¨ë‘ ìš”ì•½]ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
    else:
        st.markdown("#### ìš”ì•½/ì •ê·œí™” ê²°ê³¼")
        table_rows = []
        for n in norm:
            table_rows.append({
                "êµ­ê°€/ê´€í• ": n.get("jurisdiction") or "N/A",
                "ì§€ì—­": n.get("region") or "",
                "ë³´ê³ ì„œ ì œëª©": n.get("law_or_policy") or (n.get("title") or "N/A"),
                "ì£¼ìš” ê·œì œ ìš”ê±´": "; ".join(n.get("requirements") or []),
                "ë°œíš¨ì¼": n.get("effective_date") or "N/A",
                "ë³´ê³ ": n.get("reporting") or "N/A",
                "ì›ë¬¸": n.get("source") or n.get("link") or ""
            })
        st.dataframe(table_rows, use_container_width=True, height=min(560, 40+28*len(table_rows)))

    # ì§ì „ ì‹¤í–‰ ëŒ€ë¹„ ê°„ë‹¨ diff (ì„¸ì…˜ í•œì •)
    if prev and norm:
        st.markdown("#### ë³€í™” ë¦¬í¬íŠ¸ (ì´ë²ˆ ì‹¤í–‰ vs ì§ì „ ì‹¤í–‰)")
        def to_map(arr):
            m={}; 
            for it in arr: m[f"{it.get('jurisdiction','')}|{it.get('law_or_policy','')}"]=it
            return m
        A, B = to_map(prev), to_map(norm)
        added, removed, updated = [], [], []
        for k, v in B.items():
            if k not in A: added.append(v)
            else:
                changes=[]
                for f in ["effective_date","reporting","requirements","incentives","penalties","law_or_policy"]:
                    av = json.dumps(A[k].get(f,""), ensure_ascii=False)
                    bv = json.dumps(v.get(f,""), ensure_ascii=False)
                    if av!=bv: changes.append({"field":f,"before":A[k].get(f,""),"after":v.get(f,"")})
                if changes: updated.append({"after":v,"before":A[k],"changes":changes})
        for k, v in A.items():
            if k not in B: removed.append(v)
        st.caption(f"ì‹ ê·œ {len(added)} Â· ë³€ê²½ {len(updated)} Â· ì‚­ì œ {len(removed)}")
        colA, colB, colC = st.columns(3)
        with colA:
            st.write("**ì‹ ê·œ**"); 
            if not added: st.write("ì—†ìŒ")
            for it in added: st.markdown(f"- **{it.get('jurisdiction') or 'N/A'} Â· {it.get('law_or_policy') or 'N/A'}**")
        with colB:
            st.write("**ë³€ê²½**"); 
            if not updated: st.write("ì—†ìŒ")
            for ch in updated:
                st.markdown(f"- **{ch['after'].get('jurisdiction') or 'N/A'} Â· {ch['after'].get('law_or_policy') or 'N/A'}**")
                for c in ch["changes"]:
                    st.markdown(f"  - {c['field']}: :red[`{str(c['before'])[:80]}`] â†’ :green[`{str(c['after'])[:80]}`]")
        with colC:
            st.write("**ì‚­ì œ**"); 
            if not removed: st.write("ì—†ìŒ")
            for it in removed: st.markdown(f"- **{it.get('jurisdiction') or 'N/A'} Â· {it.get('law_or_policy') or 'N/A'}**")

st.markdown("<div class='small-muted'>Tip: Streamlit Cloudì—ì„œëŠ” Settings â†’ Secretsì— POTENS_API_KEY, POTENS_ENDPOINTë¥¼ TOMLë¡œ ì €ì¥í•˜ì„¸ìš”.</div>", unsafe_allow_html=True)
