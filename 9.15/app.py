import os
import io
import time
import urllib.request
import tempfile
import pandas as pd
import numpy as np
import streamlit as st
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.utils import embedding_functions
from utils import extract_pdf_text_with_pages, build_chunks, extract_verbatim_quotes, dedupe_preserve_order

st.set_page_config(page_title="PDF ì›ë¬¸ ë°œì·Œ QA", page_icon="ğŸ“‘", layout="wide")

DB_DIR = "db"
os.makedirs("data", exist_ok=True)
os.makedirs(DB_DIR, exist_ok=True)

# -------- Sidebar: ì—…ë¡œë“œ/ìƒ‰ì¸ --------
st.sidebar.header("â‘  ë¬¸ì„œ ì—…ë¡œë“œ & ìƒ‰ì¸")
uploaded = st.sidebar.file_uploader("PDF ì—…ë¡œë“œ(.pdf)", type=["pdf"], accept_multiple_files=True)
url_input = st.sidebar.text_input("PDF URL ë¶™ì—¬ë„£ê¸°(ì„ íƒ)")

if "embed_model" not in st.session_state:
    st.session_state.embed_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
if "chroma" not in st.session_state:
    st.session_state.client = chromadb.PersistentClient(path=DB_DIR)
    st.session_state.collection = st.session_state.client.get_or_create_collection(
        name="pdf_quotes",
        embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
    )

def index_pdf(path: str, doc_id_prefix: str):
    pages = extract_pdf_text_with_pages(path)
    chunks = build_chunks(pages, window_sentences=6, stride=3)
    if not chunks:
        return 0

    # Chromaì— ì €ì¥
    ids = []
    docs = []
    metas = []
    for i, ch in enumerate(chunks):
        ids.append(f"{doc_id_prefix}-{i}")
        docs.append(ch["text"])
        metas.append({"page": ch["page"], "source": os.path.basename(path)})

    st.session_state.collection.add(ids=ids, documents=docs, metadatas=metas)
    return len(chunks)

colL, colR = st.columns(2)
with colL:
    if uploaded:
        for f in uploaded:
            save_path = os.path.join("data", f.name)
            with open(save_path, "wb") as out:
                out.write(f.read())
            n = index_pdf(save_path, doc_id_prefix=f.name)
            st.success(f"{f.name} ìƒ‰ì¸ ì™„ë£Œ: {n}ê°œ ì²­í¬")
with colR:
    if url_input:
        try:
            tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf")
            urllib.request.urlretrieve(url_input, tmp.name)
            name = url_input.split("/")[-1].split("?")[0] or f"url_{int(time.time())}.pdf"
            save_path = os.path.join("data", name)
            os.replace(tmp.name, save_path)
            n = index_pdf(save_path, doc_id_prefix=name)
            st.success(f"URL ë¬¸ì„œ ìƒ‰ì¸ ì™„ë£Œ: {name} / {n}ê°œ ì²­í¬")
        except Exception as e:
            st.error(f"URL ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")

st.sidebar.divider()
st.sidebar.caption("â€» ìŠ¤ìº”(ì´ë¯¸ì§€) PDFëŠ” í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì´ˆê¸° ë²„ì „ì€ OCR ë¯¸ì§€ì›).")

# -------- Main: ì§ˆë¬¸ â†’ ì›ë¬¸ ë°œì·Œ --------
st.header("ğŸ“‘ PDF ì›ë¬¸ ë°œì·Œ QA (ìš”ì•½/ì°½ì‘ ê¸ˆì§€)")
query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì˜¬í•´ íƒœì–‘ê´‘ íˆ¬ì ê³„íšì€ ì–´ë–»ê²Œ ë¼?)")
top_k = st.number_input("ê²€ìƒ‰ ì²­í¬ ê°œìˆ˜ (k)", min_value=3, max_value=50, value=10)

def search_and_quote(question: str, k: int = 10, max_quotes: int = 8):
    if st.session_state.collection.count() == 0:
        return [], []
    # 1) ë²¡í„° ê²€ìƒ‰
    out = st.session_state.collection.query(query_texts=[question], n_results=k)
    docs = out.get("documents", [[]])[0]
    metas = out.get("metadatas", [[]])[0]

    # 2) ê° ì²­í¬ì—ì„œ 'ë¬¸ì¥ ë‹¨ìœ„'ë¡œ ì •í™• ì¸ìš© ì¶”ì¶œ
    quotes = []
    rows = []
    for doc, meta in zip(docs, metas):
        qts = extract_verbatim_quotes(doc, question, topk=3)
        for q in qts:
            quotes.append(q)
            rows.append({
                "ì¸ìš©ë¬¸": q,
                "í˜ì´ì§€": meta.get("page"),
                "ë¬¸ì„œ": meta.get("source")
            })
            if len(quotes) >= max_quotes:
                break
        if len(quotes) >= max_quotes:
            break

    # 3) ì¤‘ë³µ ì œê±° & ê²°ê³¼ í‘œ
    dq = dedupe_preserve_order(quotes)
    df = pd.DataFrame(rows)
    return dq, df

if st.button("ğŸ” ì›ë¬¸ ë°œì·Œ ì°¾ê¸°", type="primary"):
    if not query.strip():
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        quotes, table = search_and_quote(query, k=int(top_k))
        if not quotes:
            st.error("ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ (ë¬¸ì„œì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ê±°ë‚˜, ìŠ¤ìº” PDFë¡œ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤).")
        else:
            st.success(f"ì›ë¬¸ ì¸ìš© {len(quotes)}ê±´")
            for i, q in enumerate(quotes, start=1):
                st.markdown(f"**{i}.** â€œ{q}â€")
            st.dataframe(table, use_container_width=True)
            csv = table.to_csv(index=False).encode("utf-8-sig")
            st.download_button("â¬‡ï¸ CSVë¡œ ë‚´ë³´ë‚´ê¸°", data=csv, file_name="quotes.csv", mime="text/csv")

st.caption("â€» ë³¸ ë„êµ¬ëŠ” ì›ë¬¸ â€˜ê·¸ëŒ€ë¡œâ€™ ì¸ìš©ë§Œ ì œê³µí•©ë‹ˆë‹¤. ë¬¸ì„œì— ì—†ìœ¼ë©´ â€˜ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒâ€™ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.")
