# app.py
import os, json, time
import streamlit as st
import pandas as pd

from llm_client import LLMClient
from retriever_client import RetrieverClient
from prompts import SYSTEM_POLICY, USER_QA_TEMPLATE, USER_DIFF_TEMPLATE, CRITIC_TEMPLATE

st.set_page_config(page_title="ì‹ ì¬ìƒ ì •ì±…Â·ê·œì œ ì›ë¬¸ ì¸ìš© ê²€ìƒ‰", layout="wide")
st.title("ğŸ” ì‹ ì¬ìƒì—ë„ˆì§€ ì •ì±…Â·ê·œì œ â€” ì›ë¬¸ ì¸ìš© ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰")

# ====================== ë ˆì´ì•„ì›ƒ: ì¢Œ(2/3) PDF, ìš°(1/3) ì±—ë´‡ ======================
left, right = st.columns([2, 1], gap="large")

# ---------- ì¢Œì¸¡: PDF ë¯¸ë¦¬ë³´ê¸° ----------
with left:
    st.subheader("ğŸ“„ ì›ë¬¸ PDF ë¯¸ë¦¬ë³´ê¸°")
    pdf_files = st.file_uploader(
        "ì—¬ëŸ¬ PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (ë¯¸ë¦¬ë³´ê¸° ì „ìš© Â· ê²€ìƒ‰ ì—”ì§„ê³¼ëŠ” ë³„ê°œ)", type=["pdf"], accept_multiple_files=True
    )
    if "pdf_buffers" not in st.session_state:
        st.session_state.pdf_buffers = {}

    if pdf_files:
        names = []
        for f in pdf_files:
            names.append(f.name)
            st.session_state.pdf_buffers[f.name] = f.getvalue()
        pick = st.selectbox("ë³´ê¸° ì›í•˜ëŠ” íŒŒì¼ ì„ íƒ", names, index=0)
        try:
            st.pdf(st.session_state.pdf_buffers[pick], height=900)
        except Exception:
            st.info("ë¸Œë¼ìš°ì €/ë²„ì „ì— ë”°ë¼ ë¯¸ë¦¬ë³´ê¸°ê°€ ì œí•œë  ìˆ˜ ìˆì–´ìš”. ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë‚´ë ¤ë°›ì•„ í™•ì¸í•˜ì„¸ìš”.")
            st.download_button("ì„ íƒ PDF ë‹¤ìš´ë¡œë“œ", st.session_state.pdf_buffers[pick], file_name=pick)
    else:
        st.caption("ì—¬ê¸°ì— ì°¸ê³ ìš© PDFë¥¼ ì˜¬ë¦¬ë©´ í™”ë©´ 2/3 ì˜ì—­ì— ë¯¸ë¦¬ë³´ê¸°ê°€ í‘œì‹œë©ë‹ˆë‹¤. (í˜„ì¬ ê²€ìƒ‰ì€ íŒ€ì› RAG API ë˜ëŠ” Mock í…ìŠ¤íŠ¸ ì‚¬ìš©)")

# ---------- ìš°ì¸¡: ì±—ë´‡/ì„¤ì • ----------
with right:
    st.subheader("ğŸ§  AI ê²€ìƒ‰")

    # ===== ì„¤ì • =====
    with st.expander("ì„¤ì •", expanded=True):
        base_url = st.text_input("LLM Base URL", st.secrets.get("POTENS_BASE_URL", "https://ai.potens.ai"))
        # Potens /api/chat ë°©ì‹ì€ ëª¨ë¸ì´ í•„ìˆ˜ ì•„ë‹ˆë©´ ë¹ˆ ê°’ë„ í—ˆìš©
        model    = st.text_input("LLM Model (ì„ íƒ)", st.secrets.get("POTENS_MODEL", ""))
        retriever_url = st.text_input("Retriever Base URL", st.secrets.get("RETRIEVER_BASE_URL", ""))
        top_k = st.slider("ê²€ìƒ‰ Top-K (ìƒìœ„ ê·¼ê±° ê°œìˆ˜)", 4, 16, 8, 1)
        do_critic = st.checkbox("2ì°¨ ê²€ì¦(Critic) ì‚¬ìš©", value=True)

    # ===== ì—°ê²° ì§„ë‹¨ =====
    with st.expander("ğŸ”Œ ì—°ê²° ì§„ë‹¨(LLM Endpoint)", expanded=False):
        if st.button("ì§„ë‹¨ ì‹¤í–‰", use_container_width=True):
            llm_test = LLMClient(base_url=base_url, model=model)
            info = llm_test.diagnose()  # Potens /api/chat ì§„ë‹¨
            st.write("**/api/chat (Potens ì „ìš©)** â†’", info)

    # ë°ì´í„° ì†ŒìŠ¤ ë°°ì§€
    source_mode = "ì™¸ë¶€ RAG API" if retriever_url else "Mock(ë°ëª¨)"
    st.caption(f"ë°ì´í„° ì†ŒìŠ¤: **{source_mode}**")

    # ===== ì±— UI =====
    if "history" not in st.session_state:
        st.session_state.history = []  # [{role, content, is_diff}]
    if "pending_query" not in st.session_state:
        st.session_state.pending_query = ""

    mode = st.radio("ëª¨ë“œ", ["ì¼ë°˜ ì§ˆì˜", "ë¹„êµ ì§ˆì˜"], horizontal=True)

    # --- ì…ë ¥: ì—”í„° ì „ì†¡(chat_input) ---
    user_msg = st.chat_input("í•œêµ­ì–´ë¡œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  Enterë¥¼ ëˆ„ë¥´ì„¸ìš”. (ì˜ˆ: 2020ë…„ ì´í›„ íƒœì–‘ê´‘ ë³´ì¡°ê¸ˆ ì •ì±… ë³€í™” ìš”ì•½)")

    # ê¸°ì¡´ í…ìŠ¤íŠ¸ ë²„íŠ¼ ë°©ì‹ë„ ë³‘í–‰(ì›í•˜ë©´ ì‚¬ìš©)
    manual_query = st.text_input("ë²„íŠ¼ ë°©ì‹ ì…ë ¥ (ì˜µì…˜)", key="manual_query")
    do_run_btn = st.button("ê²€ìƒ‰ ì‹¤í–‰", use_container_width=True)

    # ì—”í„° ì „ì†¡ ë˜ëŠ” ë²„íŠ¼ í´ë¦­ì´ë©´ pending_queryì— íˆ¬ì…
    if user_msg:
        st.session_state.pending_query = user_msg
    elif do_run_btn and manual_query.strip():
        st.session_state.pending_query = manual_query.strip()

    # ==== ìœ í‹¸ ====
    def ensure_json(txt):
        s = txt.find("{"); e = txt.rfind("}")
        return txt[s:e+1] if s>=0 and e>s else txt

    def render_json_answer(data, is_diff=False):
        st.markdown("**ìš”ì•½ ë‹µë³€**")
        st.write(data.get("answer", "(answer ì—†ìŒ)"))

        if "timeline" in data:
            st.markdown("**ë³€í™” íƒ€ì„ë¼ì¸**")
            df = pd.DataFrame(data.get("timeline", []))
            if not df.empty: st.dataframe(df, use_container_width=True)

        if is_diff:
            st.markdown("**ì°¨ì´ í‘œ(diff_table)**")
            ddf = pd.DataFrame(data.get("diff_table", []))
            if not ddf.empty: st.dataframe(ddf, use_container_width=True)
            st.markdown("**ëˆ„ë½/ë¶ˆí™•ì‹¤**")
            st.write(data.get("missing_info", "(ì—†ìŒ)"))

        st.markdown("**ê·¼ê±° ì¸ìš©(ì›ë¬¸ ê·¸ëŒ€ë¡œ)**")
        qdf = pd.DataFrame(data.get("quotes", []))
        if not qdf.empty: st.dataframe(qdf, use_container_width=True)

    # ì´ì „ ëŒ€í™” ì¶œë ¥
    if st.session_state.history:
        st.divider()
        st.caption("ëŒ€í™” ê¸°ë¡")
        for turn in st.session_state.history:
            if turn["role"] == "user":
                st.write("ğŸ™‹â€â™€ï¸ ", turn["content"])
            else:
                render_json_answer(turn["content"], is_diff=turn.get("is_diff", False))

    # ===== ì‹¤í–‰ íŒŒì´í”„ë¼ì¸ =====
    if st.session_state.pending_query:
        query = st.session_state.pending_query
        st.session_state.pending_query = ""  # ì†Œë¹„

        # 0) ì‚¬ìš©ì ë©”ì‹œì§€ ê¸°ë¡
        st.session_state.history.append({"role":"user","content":query})

        # ì§„í–‰ ìƒíƒœ í‘œì‹œ
        with st.status("ê²€ìƒ‰ ì¤€ë¹„ ì¤‘...", expanded=False) as status:
            status.update(label="ğŸ” ìœ ì‚¬ ë¬¸ë‹¨ ê²€ìƒ‰ ì¤‘...", state="running")

            # 1) ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ í™•ë³´
            start = time.time()
            if retriever_url:
                retriever = RetrieverClient(base_url=retriever_url)
                chunks = retriever.search(query, k=top_k)
                def fmt(c):
                    hdr = f"[{c['doc_id']} p.{c.get('page_start','?')}-{c.get('page_end','?')} lines {c.get('line_start','?')}-{c.get('line_end','?')}]"
                    return hdr + "\n" + c["text"]
                context = "\n\n---\n\n".join(fmt(c) for c in chunks) if chunks else "(ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)"
            else:
                retriever = RetrieverClient(base_url="")
                chunks = retriever.search(query, k=top_k)
                def fmt(c):
                    hdr = f"[{c['doc_id']} p.{c.get('page_start','?')}-{c.get('page_end','?')} lines {c.get('line_start','?')}-{c.get('line_end','?')}]"
                    return hdr + "\n" + c["text"]
                context = "\n\n---\n\n".join(fmt(c) for c in chunks) if chunks else "(ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)"
            t_search = time.time() - start

            status.update(label=f"ğŸ§  LLM í˜¸ì¶œ ì¤‘... (ê²€ìƒ‰ {t_search:.1f}s)", state="running")

            # 2) í”„ë¡¬í”„íŠ¸ (í•œêµ­ì–´ ê°•ì œ)
            if mode == "ì¼ë°˜ ì§ˆì˜":
                user_prompt = USER_QA_TEMPLATE.format(question=query, context=context, k=top_k)
                is_diff = False
            else:
                user_prompt = USER_DIFF_TEMPLATE.format(question=query, context=context, k=top_k)
                is_diff = True

            # 3) LLM í˜¸ì¶œ
            llm = LLMClient(base_url=base_url, model=model)
            raw = llm.chat_json(SYSTEM_POLICY + "\nëª¨ë“  ì¶œë ¥ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.", user_prompt, temperature=0.2)

            # â”” ì—°ê²° ì˜¤ë¥˜(JSON ë¬¸ìì—´ì— _error í¬í•¨)ë©´ ê·¸ëŒ€ë¡œ í‘œê¸°í•˜ê³  ì¢…ë£Œ
            if raw.strip().startswith("{") and '"_error"' in raw:
                status.update(label="âŒ LLM í˜¸ì¶œ ì˜¤ë¥˜", state="error")
                st.error("LLM í˜¸ì¶œ ì˜¤ë¥˜ ìƒì„¸")
                st.code(raw, language="json")
                st.stop()

            raw_json = ensure_json(raw)

            # 4) íŒŒì‹±
            try:
                data = json.loads(raw_json)
            except Exception:
                data = {"answer":"JSON íŒŒì‹± ì‹¤íŒ¨. ëª¨ë¸ ì‘ë‹µ ì›ë¬¸ì„ í™•ì¸í•˜ì„¸ìš”.", "quotes":[], "raw":raw}

            # 5) Critic (ì„ íƒ)
            if do_critic:
                status.update(label="ğŸ§ª ê²€ì¦(Critic) ì¤‘...", state="running")
                critic_user = CRITIC_TEMPLATE.format(context=context, model_json=json.dumps(data, ensure_ascii=False))
                critic_raw = llm.chat_json("ë‹¹ì‹ ì€ JSON ê°ì‚¬ì§€ëŠ¥ì…ë‹ˆë‹¤. ì¶œë ¥ì€ JSONë§Œ.", critic_user, temperature=0.0)
                cjson = ensure_json(critic_raw)
                try:
                    judge = json.loads(cjson)
                    if not judge.get("is_valid", True) and judge.get("final"):
                        data = judge["final"]
                        st.toast("Critic ë³´ì • ì ìš©", icon="âœ…")
                except Exception:
                    pass

            # 6) ì¶œë ¥ & ê¸°ë¡
            st.session_state.history.append({"role":"assistant","content":data,"is_diff":is_diff})
            status.update(label="âœ… ì™„ë£Œ", state="complete")
            st.rerun()  # ê²°ê³¼ê°€ ë°”ë¡œ ìœ„ 'ëŒ€í™” ê¸°ë¡'ì— ë³´ì´ë„ë¡ ìƒˆë¡œê³ ì¹¨
